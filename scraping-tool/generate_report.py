#!/usr/bin/env python3
"""
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’ Markdown å½¢å¼ã®è¦‹ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆã«å¤‰æ›ã€‚
å‰å›çµæœã¨ã®å·®åˆ†ï¼ˆæ–°è¦ãƒ»ä¾¡æ ¼å¤‰å‹•ãƒ»å‰Šé™¤ï¼‰ã‚’æ¤œå‡ºã—ã¦è¡¨ç¤ºã€‚
æ¤œç´¢æ¡ä»¶ï¼ˆconfig.pyï¼‰ã‚’ãƒ¬ãƒãƒ¼ãƒˆã«å«ã‚ã‚‹ã€‚

ä½¿ã„æ–¹:
  python generate_report.py result.json -o report.md
  python generate_report.py result.json --compare previous.json -o report.md
"""

import argparse
import sys
from collections import defaultdict
from datetime import datetime, timezone, timedelta
from pathlib import Path

JST = timezone(timedelta(hours=9))
from typing import Any, Optional

from optional_features import optional_features
from report_utils import (
    compare_listings,
    format_address_from_ward,
    format_area,
    format_floor,
    format_price,
    format_walk,
    get_station_group,
    get_ward_from_address,
    google_maps_link,
    load_json,
    listing_key,
    normalize_listing_name,
    row_merge_key,
    format_total_units,
)

try:
    from config import (
        PRICE_MIN_MAN,
        PRICE_MAX_MAN,
        AREA_MIN_M2,
        AREA_MAX_M2,
        BUILT_YEAR_MIN,
        WALK_MIN_MAX,
        TOTAL_UNITS_MIN,
        STATION_PASSENGERS_MIN,
        AREA_LABEL,
        TOKYO_23_WARDS,
        ALLOWED_LINE_KEYWORDS,
    )
except ImportError:
    PRICE_MIN_MAN, PRICE_MAX_MAN = 7500, 10000
    AREA_MIN_M2, AREA_MAX_M2 = 65, 70
    BUILT_YEAR_MIN = datetime.now().year - 20
    WALK_MIN_MAX = 7
    TOTAL_UNITS_MIN = 100
    STATION_PASSENGERS_MIN = 0
    AREA_LABEL = "æ±äº¬23åŒº"
    ALLOWED_LINE_KEYWORDS = ()
    TOKYO_23_WARDS = (
        "åƒä»£ç”°åŒº", "ä¸­å¤®åŒº", "æ¸¯åŒº", "æ–°å®¿åŒº", "æ–‡äº¬åŒº", "å°æ±åŒº", "å¢¨ç”°åŒº", "æ±Ÿæ±åŒº",
        "å“å·åŒº", "ç›®é»’åŒº", "å¤§ç”°åŒº", "ä¸–ç”°è°·åŒº", "æ¸‹è°·åŒº", "ä¸­é‡åŒº", "æ‰ä¸¦åŒº", "è±Šå³¶åŒº",
        "åŒ—åŒº", "è’å·åŒº", "æ¿æ©‹åŒº", "ç·´é¦¬åŒº", "è¶³ç«‹åŒº", "è‘›é£¾åŒº", "æ±Ÿæˆ¸å·åŒº",
    )


def get_search_conditions_md() -> str:
    """æ¤œç´¢æ¡ä»¶ï¼ˆconfig.pyï¼‰ã‚’Markdownã®è¡¨å½¢å¼ã§å…¨ã¦åˆ—æŒ™ã€‚"""
    if PRICE_MAX_MAN >= 10000:
        price_range = f"{PRICE_MIN_MAN // 10000}å„„{PRICE_MIN_MAN % 10000}ä¸‡ã€œ{PRICE_MAX_MAN // 10000}å„„å††" if PRICE_MIN_MAN >= 10000 else f"{PRICE_MIN_MAN:,}ä¸‡ã€œ{PRICE_MAX_MAN // 10000}å„„å††"
    else:
        price_range = f"{PRICE_MIN_MAN:,}ä¸‡ã€œ{PRICE_MAX_MAN:,}ä¸‡å††"
    rows = [
        "| é …ç›® | æ¡ä»¶ |",
        "|------|------|",
        f"| æ¤œç´¢åœ°åŸŸ | {AREA_LABEL} |",
        f"| ä¾¡æ ¼ | {price_range} |",
        f"| å°‚æœ‰é¢ç© | {AREA_MIN_M2}ã€œ{AREA_MAX_M2}ã¡ |",
        "| é–“å–ã‚Š | 2LDKã€œ3LDK ç³»ï¼ˆ2LDK, 3LDK, 2DK, 3DK ãªã©ï¼‰ |",
        f"| ç¯‰å¹´ | {BUILT_YEAR_MIN}å¹´ä»¥é™ï¼ˆç¯‰20å¹´ä»¥å†…ï¼‰ |",
        f"| é§…å¾’æ­© | {WALK_MIN_MAX}åˆ†ä»¥å†… |",
        f"| ç·æˆ¸æ•° | {TOTAL_UNITS_MIN}æˆ¸ä»¥ä¸Š |",
        "| è³‡ç”£æ€§ãƒ©ãƒ³ã‚¯ | ç‹¬è‡ªã‚¹ã‚³ã‚¢ï¼ˆé§…ä¹—é™å®¢æ•°ãƒ»å¾’æ­©ãƒ»ç¯‰å¹´ãƒ»ç·æˆ¸æ•°ï¼‰4æ®µéšï¼ˆS/A/B/Cï¼‰ã€‚å‚è€ƒå€¤ã€‚ |",
        "| è¡¨ç¤ºå¯¾è±¡ | è³‡ç”£æ€§Bä»¥ä¸Šï¼ˆS/A/Bï¼‰ã®ç‰©ä»¶ã®ã¿è¡¨ç¤ºã€‚æ ¹æ‹ ã¯è¡¨ã®ã€Œè³‡ç”£æ€§æ ¹æ‹ ã€åˆ—å‚ç…§ã€‚ |",
        "| 10å¹´ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | FutureEstatePredictorï¼ˆåç›Šé‚„å…ƒãƒ»åŸä¾¡æ³•ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼‰ã«ã‚ˆã‚‹æ¥½è¦³ãƒ»ä¸­ç«‹ãƒ»æ‚²è¦³3ã‚·ãƒŠãƒªã‚ªã€‚å„åˆ—ã«ã€Œäºˆæ¸¬é‡‘é¡ï¼ˆå«ã¿ç›Š/é¨°è½ç‡ï¼‰ã€ã‚’è¡¨ç¤ºã€‚ |",
        "| ãƒ­ãƒ¼ãƒ³è©¦ç®— | 50å¹´å¤‰å‹•é‡‘åˆ©ãƒ»é ­é‡‘ãªã—ã€‚è«¸çµŒè²»ï¼ˆä¿®ç¹•ç©ç«‹ç­‰ï¼‰æœˆ3.5ä¸‡å††ã‚’åŠ ç®—ã—ãŸæœˆé¡æ”¯æ‰•ã€‚ |",
        "| é€šå‹¤æ™‚é–“ | ã‚¨ãƒ ã‚¹ãƒªãƒ¼ã‚­ãƒ£ãƒªã‚¢ï¼ˆè™ãƒé–€ï¼‰ãƒ»playgroundï¼ˆåƒä»£ç”°åŒºä¸€ç•ªç”ºï¼‰ã¾ã§ã€‚ãƒ‰ã‚¢toãƒ‰ã‚¢ï¼ˆç‰©ä»¶â†’æœ€å¯„é§…ã®å¾’æ­©ï¼‹æœ€å¯„é§…â†’ã‚ªãƒ•ã‚£ã‚¹ï¼‰ã€‚ç™»éŒ²æ¸ˆã¿é§…ã¯ãã®åˆè¨ˆã€æœªç™»éŒ²ã¯(æ¦‚ç®—)ã§å¾’æ­©ï¼‹é§…â†’ä¼šç¤¾æœ€å¯„ã‚Šé§…ï¼‹ä¼šç¤¾æœ€å¯„ã‚Šé§…â†’ä¼šç¤¾ã®å¾’æ­©ã‚’è¡¨ç¤ºã€‚ |",
    ]
    if ALLOWED_LINE_KEYWORDS:
        line_label = "ãƒ»".join(ALLOWED_LINE_KEYWORDS[:5]) + (" ãªã©" if len(ALLOWED_LINE_KEYWORDS) > 5 else "")
        rows.append(f"| è·¯ç·š | {line_label} ã«é™å®š |")
    if STATION_PASSENGERS_MIN > 0:
        rows.append(f"| é§…ä¹—é™å®¢æ•° | 1æ—¥ã‚ãŸã‚Š {STATION_PASSENGERS_MIN:,}äººä»¥ä¸Šã®é§…ã®ã¿ï¼ˆdata/station_passengers.jsonï¼‰ |")
    return "\n".join(rows)


def _is_asset_rank_b_or_above(r: dict) -> bool:
    """è³‡ç”£æ€§ãŒBä»¥ä¸Šï¼ˆS/A/Bï¼‰ã‹ã©ã†ã‹ã€‚"""
    _, rank = optional_features.get_asset_score_and_rank(r)
    return rank in ("S", "A", "B")


def _price_diff_for_sort(r: dict) -> float:
    """ç¾åœ¨ä¾¡æ ¼ã¨10å¹´å¾Œæ¨å®šä¾¡æ ¼ã®å·®é¡ï¼ˆä¸‡å††ï¼‰ã€‚å·®é¡ãŒå¤§ãã„é †ã‚½ãƒ¼ãƒˆç”¨ã€‚"""
    price_man = r.get("price_man") or 0
    sim = optional_features.simulate_10year_from_listing(r)
    price_10y = getattr(sim, "price_10y_man", 0) or 0 if sim else 0
    return price_man - price_10y


def _listing_cells(r: dict) -> dict[str, Any]:
    """1ç‰©ä»¶ã®è¡¨ç”¨ã‚»ãƒ«å€¤ã‚’ã¾ã¨ã‚ã¦è¿”ã™ã€‚è¡Œçµ„ã¿ç«‹ã¦ã®é‡è¤‡ã‚’é¿ã‘ã‚‹ã€‚"""
    _, rank, breakdown = optional_features.get_asset_score_and_rank_with_breakdown(r)
    opt_10y, neu_10y, pes_10y = optional_features.get_three_scenario_columns(r)
    monthly_loan, _ = optional_features.get_loan_display_for_listing(r.get("price_man"))
    m3_str, pg_str = optional_features.get_commute_display_with_estimate(r.get("station_line"), r.get("walk_min"))
    return {
        "rank": rank,
        "breakdown": breakdown,
        "opt_10y": opt_10y,
        "neu_10y": neu_10y,
        "pes_10y": pes_10y,
        "monthly_loan": monthly_loan,
        "m3_str": m3_str,
        "pg_str": pg_str,
        "name": (r.get("name") or "")[:30],
        "price": format_price(r.get("price_man")),
        "layout": r.get("layout", "-"),
        "area": format_area(r.get("area_m2")),
        "built": f"ç¯‰{r.get('built_year', '-')}å¹´" if r.get("built_year") else "-",
        "walk": optional_features.format_all_station_walk(r.get("station_line"), r.get("walk_min")),
        "floor_str": format_floor(r.get("floor_position"), r.get("floor_total")),
        "units": format_total_units(r.get("total_units")),
        "address_short": format_address_from_ward(r.get("address") or ""),
        "address_trunc": (r.get("address") or "")[:20],
        "gmap": google_maps_link(r.get("address") or ""),
    }


def _link_from_group(group: list[dict]) -> str:
    """åŒåãƒ»åŒä¾¡æ ¼ãƒ»åŒé–“å–ã‚Šã®ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰è©³ç´°ãƒªãƒ³ã‚¯æ–‡å­—åˆ—ã‚’çµ„ã¿ç«‹ã¦ã‚‹ã€‚"""
    urls = [x.get("url", "") for x in group if x.get("url")]
    if len(urls) == 1:
        return f"[è©³ç´°]({urls[0]})"
    link = " ".join(f"[{i+1}]({u})" for i, u in enumerate(urls[:3]))
    if len(urls) > 3:
        link += f" ä»–{len(urls)-3}ä»¶"
    return link


def generate_markdown(
    listings: list[dict[str, Any]],
    diff: Optional[dict[str, Any]] = None,
    output_path: Optional[Path] = None,
    report_url: Optional[str] = None,
) -> str:
    """Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã€‚è³‡ç”£æ€§Bä»¥ä¸Šã®ç‰©ä»¶ã®ã¿è¡¨ç¤ºã—ã€æ ¹æ‹ åˆ—ã‚’è¿½åŠ ã€‚"""
    now = datetime.now(JST).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
    search_conditions = get_search_conditions_md()

    # è³‡ç”£æ€§Bä»¥ä¸Šã«çµã‚‹
    listings_a = [r for r in listings if _is_asset_rank_b_or_above(r)]
    diff_a: Optional[dict[str, Any]] = None
    if diff:
        diff_a = {
            "new": [r for r in diff.get("new", []) if _is_asset_rank_b_or_above(r)],
            "updated": [item for item in diff.get("updated", []) if _is_asset_rank_b_or_above(item.get("current", {}))],
            "removed": [r for r in diff.get("removed", []) if _is_asset_rank_b_or_above(r)],
        }

    lines = [
        "# ä¸­å¤ãƒãƒ³ã‚·ãƒ§ãƒ³ç‰©ä»¶ä¸€è¦§ãƒ¬ãƒãƒ¼ãƒˆ",
        "",
    ]
    if report_url and report_url.strip():
        lines.extend([
            f"**ãƒ¬ãƒãƒ¼ãƒˆï¼ˆGitHubï¼‰**: [results/report ã‚’é–‹ã]({report_url.strip()})",
            "",
        ])
    lines.extend([
        "## ğŸ” æ¤œç´¢æ¡ä»¶ï¼ˆä¸€è¦§ï¼‰",
        "",
        "ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ä»¥ä¸‹ã®æ¡ä»¶ã§æ¤œç´¢ãƒ»å–å¾—ã—ãŸç‰©ä»¶ã§ã™ã€‚**è³‡ç”£æ€§Bä»¥ä¸Šã®ã¿è¡¨ç¤º**ã€‚",
        "",
        search_conditions,
        "",
        "---",
        "",
        f"**æ›´æ–°æ—¥æ™‚**: {now}ï¼ˆJSTï¼‰",
        f"**å¯¾è±¡ä»¶æ•°**: {len(listings_a)}ä»¶ï¼ˆè³‡ç”£æ€§Bä»¥ä¸Š / å…¨{len(listings)}ä»¶ä¸­ï¼‰",
        "",
    ])

    # æ–°è¦ç‰©ä»¶ï¼ˆåŒºã«é–¢ä¿‚ãªãä¸€ç•ªä¸Šã«è¡¨ç¤ºã€‚åŒåãƒ»åŒä¾¡æ ¼ãƒ»åŒé–“å–ã‚Šã¯1è¡Œã«ã¾ã¨ã‚ã‚‹ï¼‰
    if diff_a and diff_a["new"]:
        m3_label, pg_label = optional_features.get_destination_labels()
        lines.extend([
            "## ğŸ†• æ–°è¦ç‰©ä»¶",
            "",
            f"| ç‰©ä»¶å | ä¾¡æ ¼ | é–“å–ã‚Š | å°‚æœ‰ | ç¯‰å¹´ | é§…å¾’æ­© | éš | ç·æˆ¸æ•° | è³‡ç”£æ€§(S/A/B/C) | è³‡ç”£æ€§æ ¹æ‹  | æ¥½è¦³10å¹´å¾Œ | ä¸­ç«‹10å¹´å¾Œ | æ‚²è¦³10å¹´å¾Œ | æœˆé¡(50å¹´ãƒ»è«¸çµŒè²»3.5ä¸‡) | {m3_label} | {pg_label} | æ‰€åœ¨åœ° | Google Map | è©³ç´° |",
            f"|--------|------|--------|------|------|--------|-----|--------|----------------|------------|------------|------------|------------|------------------------|------|------|--------|------------|------|",
        ])
        new_groups: dict[tuple, list[dict]] = defaultdict(list)
        for r in diff_a["new"]:
            new_groups[row_merge_key(r)].append(r)
        for group in sorted(new_groups.values(), key=lambda g: _price_diff_for_sort(g[0]), reverse=True):
            r = group[0]
            c = _listing_cells(r)
            link = _link_from_group(group)
            lines.append(f"| {c['name']} | {c['price']} | {c['layout']} | {c['area']} | {c['built']} | {c['walk']} | {c['floor_str']} | {c['units']} | {c['rank']} | {c['breakdown']} | {c['opt_10y']} | {c['neu_10y']} | {c['pes_10y']} | {c['monthly_loan']} | {c['m3_str']} | {c['pg_str']} | {c['address_trunc']} | {c['gmap']} | {link} |")
        lines.append("")

    # å¤‰æ›´ã‚µãƒãƒªãƒ¼ï¼ˆæ–°è¦ãƒ»ä¾¡æ ¼å¤‰å‹•ãƒ»å‰Šé™¤ã®ä»¶æ•°ï¼‰
    if diff_a:
        new_count = len(diff_a["new"])
        updated_count = len(diff_a["updated"])
        removed_count = len(diff_a["removed"])
        if new_count > 0 or updated_count > 0 or removed_count > 0:
            lines.extend([
                "## ğŸ“Š å¤‰æ›´ã‚µãƒãƒªãƒ¼",
                "",
                f"- ğŸ†• **æ–°è¦**: {new_count}ä»¶",
                f"- ğŸ”„ **ä¾¡æ ¼å¤‰å‹•**: {updated_count}ä»¶",
                f"- âŒ **å‰Šé™¤**: {removed_count}ä»¶",
                "",
            ])

    # ä¾¡æ ¼å¤‰å‹•
    if diff_a and diff_a["updated"]:
        m3_label, pg_label = optional_features.get_destination_labels()
        lines.extend([
            "## ğŸ”„ ä¾¡æ ¼å¤‰å‹•",
            "",
            f"| ç‰©ä»¶å | å¤‰æ›´å‰ | å¤‰æ›´å¾Œ | å·®é¡ | é–“å–ã‚Š | å°‚æœ‰ | éš | ç·æˆ¸æ•° | è³‡ç”£æ€§(S/A/B/C) | è³‡ç”£æ€§æ ¹æ‹  | æ¥½è¦³10å¹´å¾Œ | ä¸­ç«‹10å¹´å¾Œ | æ‚²è¦³10å¹´å¾Œ | æœˆé¡(50å¹´ãƒ»è«¸çµŒè²»3.5ä¸‡) | {m3_label} | {pg_label} | Google Map | è©³ç´°URL |",
            f"|--------|--------|--------|------|--------|------|-----|--------|----------------|------------|------------|------------|------------|------------------------|------|------|------------|---------|",
        ])
        for item in sorted(diff_a["updated"], key=lambda x: _price_diff_for_sort(x["current"]), reverse=True):
            curr = item["current"]
            prev = item["previous"]
            c = _listing_cells(curr)
            prev_price = format_price(prev.get("price_man"))
            curr_price = c["price"]
            diff_price = (curr.get("price_man") or 0) - (prev.get("price_man") or 0)
            diff_str = f"{'+' if diff_price >= 0 else ''}{diff_price}ä¸‡å††" if diff_price != 0 else "å¤‰å‹•ãªã—"
            url = curr.get("url", "")
            lines.append(f"| {c['name']} | {prev_price} | {curr_price} | {diff_str} | {c['layout']} | {c['area']} | {c['floor_str']} | {c['units']} | {c['rank']} | {c['breakdown']} | {c['opt_10y']} | {c['neu_10y']} | {c['pes_10y']} | {c['monthly_loan']} | {c['m3_str']} | {c['pg_str']} | {c['gmap']} | [è©³ç´°]({url}) |")
        lines.append("")

    # å‰Šé™¤ã•ã‚ŒãŸç‰©ä»¶
    if diff_a and diff_a["removed"]:
        m3_label, pg_label = optional_features.get_destination_labels()
        lines.extend([
            "## âŒ å‰Šé™¤ã•ã‚ŒãŸç‰©ä»¶",
            "",
            f"| ç‰©ä»¶å | ä¾¡æ ¼ | é–“å–ã‚Š | å°‚æœ‰ | éš | ç·æˆ¸æ•° | è³‡ç”£æ€§(S/A/B/C) | è³‡ç”£æ€§æ ¹æ‹  | æ¥½è¦³10å¹´å¾Œ | ä¸­ç«‹10å¹´å¾Œ | æ‚²è¦³10å¹´å¾Œ | æœˆé¡(50å¹´ãƒ»è«¸çµŒè²»3.5ä¸‡) | {m3_label} | {pg_label} | Google Map | è©³ç´°URL |",
            f"|--------|------|--------|------|-----|--------|----------------|------------|------------|------------|------------|------------------------|------|------|------------|---------|",
        ])
        for r in sorted(diff_a["removed"], key=_price_diff_for_sort, reverse=True):
            c = _listing_cells(r)
            url = r.get("url", "")
            lines.append(f"| {c['name']} | {c['price']} | {c['layout']} | {c['area']} | {c['floor_str']} | {c['units']} | {c['rank']} | {c['breakdown']} | {c['opt_10y']} | {c['neu_10y']} | {c['pes_10y']} | {c['monthly_loan']} | {c['m3_str']} | {c['pg_str']} | {c['gmap']} | [è©³ç´°]({url}) |")
        lines.append("")

    # å…¨ç‰©ä»¶ä¸€è¦§: åŒºã”ã¨ â†’ æœ€å¯„é§…ã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆè³‡ç”£æ€§Bä»¥ä¸Šã®ç‰©ä»¶ã®ã¿ï¼‰
    lines.append("## ğŸ“‹ ç‰©ä»¶ä¸€è¦§ï¼ˆåŒºãƒ»æœ€å¯„é§…åˆ¥ãƒ»è³‡ç”£æ€§Bä»¥ä¸Šï¼‰")
    lines.append("")

    # åŒº â†’ æœ€å¯„é§… â†’ ç‰©ä»¶ãƒªã‚¹ãƒˆã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆåŒºã®é †åºã¯ TOKYO_23_WARDSï¼‰
    ward_order = {w: i for i, w in enumerate(TOKYO_23_WARDS)}
    by_ward: dict[str, list[dict]] = {}
    no_ward: list[dict] = []
    for r in listings_a:
        ward = get_ward_from_address(r.get("address") or "")
        if ward:
            by_ward.setdefault(ward, []).append(r)
        else:
            no_ward.append(r)

    # åŒºã‚’ TOKYO_23_WARDS ã®é †ã§ã€ãã®å¾Œã€Œãã®ä»–ã€
    ordered_wards = sorted(by_ward.keys(), key=lambda w: ward_order.get(w, 999))
    if no_ward:
        ordered_wards.append("(åŒºä¸æ˜)")
        by_ward["(åŒºä¸æ˜)"] = no_ward

    for ward in ordered_wards:
        ward_listings = by_ward.get(ward, [])
        if not ward_listings:
            continue
        lines.append(f"### {ward}")
        lines.append("")

        # æœ€å¯„é§…ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        by_station: dict[str, list[dict]] = {}
        for r in ward_listings:
            st = get_station_group(r.get("station_line") or "")
            by_station.setdefault(st, []).append(r)

        for station in sorted(by_station.keys()):
            st_listings = by_station[station]
            lines.append(f"#### {station}")
            # ãã®é§…ã‚°ãƒ«ãƒ¼ãƒ—ã®æ‰€åœ¨åœ°ï¼ˆåŒºä»¥é™ï¼‰ã‚’é‡è¤‡é™¤ã„ã¦åˆ—æŒ™
            addrs = []
            seen: set[str] = set()
            for r in st_listings:
                a = format_address_from_ward(r.get("address") or "")
                if a != "-" and a not in seen:
                    seen.add(a)
                    addrs.append(a)
            if addrs:
                lines.append("æ‰€åœ¨åœ°: " + "ã€".join(addrs[:5]) + (" ä»–" if len(addrs) > 5 else ""))
            lines.append("")
            m3_label, pg_label = optional_features.get_destination_labels()
            lines.append(f"| ç‰©ä»¶å | ä¾¡æ ¼ | é–“å–ã‚Š | å°‚æœ‰ | ç¯‰å¹´ | é§…å¾’æ­© | æ‰€åœ¨åœ° | Google Map | éš | ç·æˆ¸æ•° | è³‡ç”£æ€§(S/A/B/C) | è³‡ç”£æ€§æ ¹æ‹  | æ¥½è¦³10å¹´å¾Œ | ä¸­ç«‹10å¹´å¾Œ | æ‚²è¦³10å¹´å¾Œ | æœˆé¡(50å¹´ãƒ»è«¸çµŒè²»3.5ä¸‡) | {m3_label} | {pg_label} | è©³ç´° |")
            lines.append("|--------|------|--------|------|------|--------|--------|------------|-----|--------|----------------|------------|------------|------------|------------|------------------------|------|------|------|")

            # åŒåãƒ»åŒä¾¡æ ¼ãƒ»åŒé–“å–ã‚Šã§1è¡Œã«ã¾ã¨ã‚ã‚‹ã€‚ç¾åœ¨ä¾¡æ ¼ã¨10å¹´å¾Œæ¨å®šä¾¡æ ¼ã®å·®é¡ãŒå¤§ãã„é †ã«è¡¨ç¤º
            merge_groups: dict[tuple, list[dict]] = defaultdict(list)
            for r in st_listings:
                merge_groups[row_merge_key(r)].append(r)
            for group in sorted(merge_groups.values(), key=lambda g: _price_diff_for_sort(g[0]), reverse=True):
                r = group[0]
                c = _listing_cells(r)
                link = _link_from_group(group)
                lines.append(f"| {c['name']} | {c['price']} | {c['layout']} | {c['area']} | {c['built']} | {c['walk']} | {c['address_short']} | {c['gmap']} | {c['floor_str']} | {c['units']} | {c['rank']} | {c['breakdown']} | {c['opt_10y']} | {c['neu_10y']} | {c['pes_10y']} | {c['monthly_loan']} | {c['m3_str']} | {c['pg_str']} | {link} |")
            lines.append("")

    lines.extend([
        "---",
        "",
        f"*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚: {now}ï¼ˆJSTï¼‰*",
    ])

    content = "\n".join(lines)
    if output_path:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_path}", file=sys.stderr)
    return content


def main() -> None:
    ap = argparse.ArgumentParser(description="ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’Markdownãƒ¬ãƒãƒ¼ãƒˆã«å¤‰æ›")
    ap.add_argument("input", type=Path, help="å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmain.pyã®å‡ºåŠ›ï¼‰")
    ap.add_argument("--compare", "-c", type=Path, help="å‰å›çµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå·®åˆ†æ¤œå‡ºç”¨ï¼‰")
    ap.add_argument("--output", "-o", type=Path, help="å‡ºåŠ›Markdownãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœªæŒ‡å®šæ™‚ã¯stdoutï¼‰")
    ap.add_argument("--report-url", type=str, default=None, help="GitHub ã® results/report ã¸ã®URLï¼ˆæŒ‡å®šæ™‚ã®ã¿ãƒ¬ãƒãƒ¼ãƒˆå…ˆé ­ã«ãƒªãƒ³ã‚¯ã‚’è¨˜è¼‰ï¼‰")
    args = ap.parse_args()

    current = load_json(args.input)
    previous = load_json(args.compare) if args.compare and args.compare.exists() else None

    diff = compare_listings(current, previous) if previous else None
    content = generate_markdown(current, diff, args.output, report_url=args.report_url)

    if not args.output:
        print(content)


if __name__ == "__main__":
    main()
